#!/bin/bash

#BSUB -P CSC378
#BSUB -W 0:30
#BSUB -nnodes 1
#BSUB -J mimiccxr_densenet
#BSUB -o output/job%J.o
#BSUB -e output/job%J.e
#BSUB -alloc_flags "gpumps"

#BSUB -q batch
#BSUB -rn

module load ibm-wml-ce/1.7.0-2
conda activate powerai170

# Be verbose but _after_ loading modules so we don't spam the logs
set -ex

# remove the batch node so we only include compute nodes
NNODES=$(echo $LSB_HOSTS | tr ' ' '\n' | sort -u | grep -v batch | wc -l)
echo "Running job with ${NNODES} compute nodes"

# for saving model weights and so forth
export TORCH_HOME=$PROJWORK/csc378/4jh/torch_home
mkdir -p $TORCH_HOME

SEED=0
FOLD=0
JOBDIR=output/job${LSB_JOBID}/seed${SEED}/fold${FOLD}
mkdir -p $JOBDIR

jsrun -n${NNODES} -r1 -a1 -g6 -c42 \
    -E TORCH_HOME \
    $(which python) -m torch.distributed.launch --nproc_per_node=6 \
      train_densenet.py \
        -o $JOBDIR \
        --learning-rate 1e-2 \
        --hide-progress \
        --image-subdir files256x256 \
        --distributed-data-parallel \
        --batch-size $((50)) \
        #--val-iters 200 \
