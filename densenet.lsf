#!/bin/bash

#BSUB -P CSC378
#BSUB -W 0:30
#BSUB -nnodes 1
#BSUB -J mimiccxr_densenet
#BSUB -o output/job%J.o
#BSUB -e output/job%J.e
#BSUB -alloc_flags "gpumps"

#BSUB -q batch
#BSUB -rn

module load ibm-wml-ce/1.7.0-2
conda activate powerai170

# Be verbose but _after_ loading modules so we don't spam the logs
set -ex

# remove the batch node so we only include compute nodes
NNODES=$(echo $LSB_HOSTS | tr ' ' '\n' | sort -u | grep -v batch | wc -l)
echo "Running job with ${NNODES} compute nodes"

# for saving model weights and so forth
export TORCH_HOME=$PROJWORK/csc378/4jh/torch_home
mkdir -p $TORCH_HOME

SEED=0
FOLD=0
JOBDIR=output/job$(date +%y%m%d%H%M).${LSB_JOBID}
FOLDDIR=$JOBDIR/seed${SEED}/fold${FOLD}
mkdir -p $FOLDDIR

BS=50
SZ=256
LR=1e-2

echo "n${NNODES} lr${LR} ${SZ}^2 bs${BS}" > $JOBDIR/description

jsrun -n${NNODES} -r1 -a1 -g6 -c42 \
    -E OMP_NUM_THREADS=1 \
    -E TORCH_HOME \
    $(which python) -m torch.distributed.launch --nproc_per_node=6 \
      train_densenet.py \
        --outputdir $FOLDDIR \
        --learning-rate $LR \
        --hide-progress \
        --image-subdir files${SZ}x${SZ} \
        --distributed-data-parallel \
        --batch-size $BS \
        #--val-iters 200 \
