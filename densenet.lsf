#BSUB -P CSC378
#BSUB -J mimiccxr_densenet
#BSUB -o output/job%J.o
#BSUB -e output/job%J.e
#BSUB -csm n

#BSUB -q killable
#BSUB -rn

#BSUB -W 0:20
#BSUB -nnodes 4

# ATTENTION: PLEASE SET THIS TO MATCH THE VALUE OF -nnodes ABOVE
NNODES=4

module load ibm-wml-ce/1.7.0-2
conda activate powerai170

# Be verbose but _after_ loading modules so we don't spam the logs
set -ex

# remove the batch node so we only include compute nodes
#NNODES=$(echo $LSB_HOSTS | tr ' ' '\n' | sort -u | grep -v batch | wc -l)
echo "Running job with ${NNODES} compute nodes"

# for saving model weights and so forth
export TORCH_HOME=$PROJWORK/csc378/4jh/torch_home
mkdir -p $TORCH_HOME

SEED=0
FOLD=0
JOBDIR=output/job${LSB_JOBID}
FOLDDIR=$JOBDIR/seed${SEED}/fold${FOLD}
mkdir -p $FOLDDIR

BS=50
SZ=256
LR=1e-2

# set master for torch.distributed
export MASTER_ADDR=$(getent hosts $(echo $LSB_MCPU_HOSTS | awk '{print $3}') | awk '{print $1}')
export MASTER_PORT=29500

echo "n${NNODES} lr${LR} ${SZ}^2 bs${BS}" > $JOBDIR/description

jsrun \
    -n$((NNODES*6)) -r6 -a1 -g1 -c7 \
    --bind=proportional-packed:7 --launch_distribution=packed \
    -E MASTER_ADDR \
    -E MASTER_PORT \
    -E TORCH_HOME \
    ./mpienv.sh \
      $(which python) train_densenet.py \
        --outputdir $FOLDDIR \
        --learning-rate $LR \
        --hide-progress \
        --image-subdir files${SZ}x${SZ} \
        --distributed-data-parallel \
        --batch-size $BS \
        #--val-iters 200 \
